{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NL2Declare: External Validation â€” Part A (Comparison to van der Aa 2019)\n",
        "\n",
        "This notebook evaluates our fine-tuned model against the state-of-the-art approach by van der Aa (2019) on two datasets (V1 and V2), following the paper:\n",
        "- Templates considered: Init, End, Precedence, Response\n",
        "- Reported metric: Template accuracy (per-template and overall)\n",
        "- Inference format: QA-style prompt with `max_new_tokens=15`\n",
        "- Only fine-tuned adapters are supported here\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install -e ..[train]\n",
        "import os, sys, csv\n",
        "from pathlib import Path\n",
        "repo_root = Path(__file__).resolve().parents[1]\n",
        "sys.path.append(str(repo_root / \"src\"))\n",
        "\n",
        "from text2declare.inference import load_peft_model, predict_constraints_qa\n",
        "from text2declare.parsing import extract_first_constraint\n",
        "\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\")\n",
        "MODEL_ID = \"google/gemma-7b\"\n",
        "ADAPTER_DIR = str(repo_root / \"outputs/external_ft/adapter\")  # change to your adapter path\n",
        "\n",
        "# Choose dataset: V1 or V2 (both contain only Init, End, Precedence, Response)\n",
        "TEST_CSV = str(repo_root / \"data/external_validation/baseline_declare_extraction_van_der_Aa_2019/test_set_V1.csv\")\n",
        "# TEST_CSV = str(repo_root / \"data/external_validation/baseline_declare_extraction_van_der_Aa_2019/test_set_V2.csv\")\n",
        "\n",
        "# Output path for our LLM predictions (Gemma7) to mirror repo structure\n",
        "RESULTS_DIR = repo_root / \"external_validation/part_a/results\"\n",
        "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load adapter\n",
        "assert ADAPTER_DIR and len(ADAPTER_DIR) > 0\n",
        "model, tokenizer = load_peft_model(MODEL_ID, ADAPTER_DIR, hf_token=HF_TOKEN)\n",
        "\n",
        "# Read sentences and gold\n",
        "sentences, gold = [], []\n",
        "with open(TEST_CSV, newline=\"\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    header = next(reader)\n",
        "    for row in reader:\n",
        "        sentences.append(row[0])\n",
        "        gold.append(row[1])\n",
        "\n",
        "raw_outputs = predict_constraints_qa(model, tokenizer, sentences, max_new_tokens=15)\n",
        "parsed = [extract_first_constraint(o) or \"\" for o in raw_outputs]\n",
        "\n",
        "# Write predictions for archival under part_a/results\n",
        "out_name = \"results_V1.csv\" if TEST_CSV.endswith(\"V1.csv\") else \"results_V2.csv\"\n",
        "with open(RESULTS_DIR / out_name, \"w\", newline=\"\") as f:\n",
        "    w = csv.writer(f)\n",
        "    w.writerow([\"Text description\", \"Output (raw)\"])\n",
        "    for s, p in zip(sentences, parsed):\n",
        "        w.writerow([s, p])\n",
        "\n",
        "len(parsed), parsed[:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute per-template and overall template accuracy (Init, End, Response, Precedence)\n",
        "from collections import defaultdict\n",
        "\n",
        "per_templ_total = defaultdict(int)\n",
        "per_templ_correct = defaultdict(int)\n",
        "\n",
        "for g, p in zip(gold, parsed):\n",
        "    templ_g = g.split('(')[0]\n",
        "    templ_p = p.split('(')[0] if p else \"\"\n",
        "    per_templ_total[templ_g] += 1\n",
        "    if templ_g == templ_p:\n",
        "        per_templ_correct[templ_g] += 1\n",
        "\n",
        "overall_total = sum(per_templ_total.values())\n",
        "overall_correct = sum(per_templ_correct.values())\n",
        "\n",
        "per_templ_acc = {k: (per_templ_correct[k] / per_templ_total[k] if per_templ_total[k] else 0.0) for k in per_templ_total}\n",
        "overall_acc = overall_correct / overall_total if overall_total else 0.0\n",
        "\n",
        "dataset_version = \"V1\" if TEST_CSV.endswith(\"V1.csv\") else \"V2\"\n",
        "print(f\"=== Template Accuracy Results (Dataset {dataset_version}) ===\")\n",
        "print(f\"Overall Template Accuracy: {overall_acc:.3f} ({overall_correct}/{overall_total})\")\n",
        "print(\"\\nPer-template accuracy:\")\n",
        "for template in sorted(per_templ_acc.keys()):\n",
        "    acc = per_templ_acc[template]\n",
        "    count = per_templ_total[template]\n",
        "    correct = per_templ_correct[template]\n",
        "    print(f\"  {template}: {acc:.3f} ({correct}/{count})\")\n",
        "\n",
        "print(f\"\\nPredictions saved to: {RESULTS_DIR / out_name}\")\n",
        "print(\"For comparison with saved outputs, use: python analysis/external_part_a_eval.py\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
