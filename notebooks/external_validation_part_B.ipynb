{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NL2Declare: External Validation â€” Part B (Prompting Baselines vs Fine-Tuned)\n",
        "\n",
        "This notebook compares our fine-tuned approach (QA format) against prompting baselines on the same set of templates as in the paper (Init, End, AtMostOne, AtLeastOne, Response, Precedence, ChainResponse, ChainPrecedence, RespondedExistence, CoExistence, NotCoExistence).\n",
        "\n",
        "We report template accuracy and F1 as needed, but the paper emphasizes template accuracy for clean comparison.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install -e ..[train]\n",
        "import os, sys, csv\n",
        "from pathlib import Path\n",
        "repo_root = Path(__file__).resolve().parents[1]\n",
        "sys.path.append(str(repo_root / \"src\"))\n",
        "\n",
        "from text2declare.inference import load_peft_model, predict_constraints_qa\n",
        "from text2declare.parsing import extract_first_constraint\n",
        "from text2declare.evaluation import evaluate\n",
        "\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\")\n",
        "MODEL_ID = \"google/gemma-7b\"\n",
        "ADAPTER_DIR = str(repo_root / \"outputs/external_ft/adapter\")  # change to your adapter path\n",
        "\n",
        "# Use the external validation test set with 11 templates\n",
        "TEST_CSV = str(repo_root / \"data/external_validation/baselines_prompting/test_data.csv\")\n",
        "\n",
        "# Output path for our LLM predictions\n",
        "RESULTS_DIR = repo_root / \"external_validation/part_b/results\"\n",
        "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load adapter and run QA inference\n",
        "assert ADAPTER_DIR and len(ADAPTER_DIR) > 0\n",
        "model, tokenizer = load_peft_model(MODEL_ID, ADAPTER_DIR, hf_token=HF_TOKEN)\n",
        "\n",
        "sentences, gold = [], []\n",
        "with open(TEST_CSV, newline=\"\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    header = next(reader)\n",
        "    for row in reader:\n",
        "        sentences.append(row[0])\n",
        "        gold.append(row[1] if len(row) > 1 else \"\")\n",
        "\n",
        "raw_outputs = predict_constraints_qa(model, tokenizer, sentences, max_new_tokens=15)\n",
        "parsed = [extract_first_constraint(o) or \"\" for o in raw_outputs]\n",
        "\n",
        "# Write predictions to part_b/results per repo convention\n",
        "out_name = \"results_test_data_LLM.csv\"\n",
        "with open(RESULTS_DIR / out_name, \"w\", newline=\"\") as f:\n",
        "    w = csv.writer(f)\n",
        "    w.writerow([\"Text description\", \"Output (raw)\"])\n",
        "    for s, p in zip(sentences, parsed):\n",
        "        w.writerow([s, p])\n",
        "\n",
        "len(parsed), parsed[:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick template accuracy evaluation\n",
        "from collections import defaultdict\n",
        "\n",
        "per_templ_total = defaultdict(int)\n",
        "per_templ_correct = defaultdict(int)\n",
        "for g, p in zip(gold, parsed):\n",
        "    templ_g = (g.split('(')[0] if g else \"\")\n",
        "    templ_p = (p.split('(')[0] if p else \"\")\n",
        "    if templ_g:\n",
        "        per_templ_total[templ_g] += 1\n",
        "        if templ_g == templ_p:\n",
        "            per_templ_correct[templ_g] += 1\n",
        "\n",
        "overall_total = sum(per_templ_total.values())\n",
        "overall_correct = sum(per_templ_correct.values())\n",
        "per_templ_acc = {k: (per_templ_correct[k] / per_templ_total[k] if per_templ_total[k] else 0.0) for k in per_templ_total}\n",
        "overall_acc = overall_correct / overall_total if overall_total else 0.0\n",
        "\n",
        "print(f\"Template Accuracy: {overall_acc:.3f} ({overall_correct}/{overall_total})\")\n",
        "print(\"\\nPer-template accuracy:\")\n",
        "for template, acc in sorted(per_templ_acc.items()):\n",
        "    count = per_templ_total[template]\n",
        "    correct = per_templ_correct[template]\n",
        "    print(f\"  {template}: {acc:.3f} ({correct}/{count})\")\n",
        "\n",
        "print(f\"\\nPredictions saved to: {RESULTS_DIR / 'results_test_data_LLM.csv'}\")\n",
        "print(\"For comprehensive metrics comparison, use: python analysis/external_part_b_eval.py\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
