{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NL2Declare: Internal Validation\n",
        "\n",
        "This notebook performs 10-fold cross-validation on `data/internal_validation/data_crossvalidation.csv` and compares multiple decoder-only LLMs (7â€“8B class plus small Llama-3.2 1B/3B) with identical training hyperparameters, as described in the paper. Training is executed per fold and per model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install -e ..[train]\n",
        "import os, sys\n",
        "from pathlib import Path\n",
        "from statistics import mean\n",
        "\n",
        "repo_root = Path(__file__).resolve().parents[1]\n",
        "sys.path.append(str(repo_root / \"src\"))\n",
        "\n",
        "from text2declare.training import TrainConfig, train\n",
        "from text2declare.inference import load_model, load_peft_model, predict_constraints\n",
        "from text2declare.parsing import extract_first_constraint\n",
        "from text2declare.evaluation import evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Authentication and dataset path only (training/eval configured in CV cell)\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\")\n",
        "CSV_PATH = str(repo_root / \"data/internal_validation/data_crossvalidation.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 10-fold cross-validation (training per model and fold)\n",
        "# Identical hyperparameters across models. Writes per-fold outputs to outputs/internal_cv/.\n",
        "\n",
        "import csv\n",
        "from sklearn.model_selection import KFold\n",
        "import pandas as pd\n",
        "\n",
        "NUM_FOLDS = 10\n",
        "MODEL_IDS = [\n",
        "    \"google/gemma-7b\",\n",
        "    \"mistralai/Mistral-7B-v0.1\",\n",
        "    \"tiiuae/falcon-7b\",\n",
        "    \"meta-llama/Llama-2-7b-hf\",\n",
        "    \"meta-llama/Meta-Llama-3-8B\",\n",
        "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "]\n",
        "HP = dict(per_device_train_batch_size=2, gradient_accumulation_steps=4, warmup_steps=2, max_steps=200, learning_rate=2e-4)\n",
        "\n",
        "_df = pd.read_csv(CSV_PATH)\n",
        "texts = _df.iloc[:, 0].tolist()\n",
        "labels = _df.iloc[:, 1].tolist()\n",
        "\n",
        "kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "results = []\n",
        "for model_id in MODEL_IDS:\n",
        "    fold_f1_a2 = []\n",
        "    fold_f1_a4 = []\n",
        "    fold_tacc = []\n",
        "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(texts)):\n",
        "        fold_dir = repo_root / f\"outputs/internal_cv/{model_id.split('/')[-1]}_fold_{fold_idx}\"\n",
        "        train_csv = fold_dir / \"train.csv\"\n",
        "        test_csv = fold_dir / \"test.csv\"\n",
        "        fold_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        with open(train_csv, \"w\", newline=\"\") as f:\n",
        "            w = csv.writer(f)\n",
        "            w.writerow([\"Text Description\", \"Declare Constraint\"])  # headers expected by loader\n",
        "            for i in train_idx:\n",
        "                w.writerow([texts[i], labels[i]])\n",
        "        with open(test_csv, \"w\", newline=\"\") as f:\n",
        "            w = csv.writer(f)\n",
        "            w.writerow([\"Text Description\", \"Declare Constraint\"])  # GT for evaluation\n",
        "            for i in test_idx:\n",
        "                w.writerow([texts[i], labels[i]])\n",
        "\n",
        "        cfg_fold = TrainConfig(model_id=model_id, train_csv=str(train_csv), output_dir=str(fold_dir), hf_token=HF_TOKEN, **HP)\n",
        "        train(cfg_fold)\n",
        "\n",
        "        # Load fine-tuned adapter and run inference (QA-style or few-shot format)\n",
        "        model, tokenizer = load_peft_model(model_id, str(fold_dir / \"adapter\"), hf_token=HF_TOKEN)\n",
        "        test_sentences = [texts[i] for i in test_idx]\n",
        "        raw_out = predict_constraints(model, tokenizer, test_sentences, style=\"few_shot\")\n",
        "        parsed = [extract_first_constraint(o) or \"\" for o in raw_out]\n",
        "\n",
        "        pred_csv = fold_dir / \"pred.csv\"\n",
        "        with open(pred_csv, \"w\", newline=\"\") as f:\n",
        "            w = csv.writer(f)\n",
        "            w.writerow([\"Text description\", \"Output (raw)\"])\n",
        "            for s, p in zip(test_sentences, parsed):\n",
        "                w.writerow([s, p])\n",
        "\n",
        "        # Compute metrics for alpha=2 and alpha=4; template accuracy derived from template errors\n",
        "        p2, r2, f12, n2, te2 = evaluate(str(test_csv), str(pred_csv), alpha=2.0)\n",
        "        p4, r4, f14, n4, te4 = evaluate(str(test_csv), str(pred_csv), alpha=4.0)\n",
        "        tacc = 1.0 - (te2 / n2 if n2 else 0.0)\n",
        "        fold_f1_a2.append(f12)\n",
        "        fold_f1_a4.append(f14)\n",
        "        fold_tacc.append(tacc)\n",
        "\n",
        "    avg_f1_a2 = mean(fold_f1_a2) if fold_f1_a2 else 0.0\n",
        "    avg_f1_a4 = mean(fold_f1_a4) if fold_f1_a4 else 0.0\n",
        "    avg_tacc = mean(fold_tacc) if fold_tacc else 0.0\n",
        "    results.append((model_id, avg_f1_a2, avg_f1_a4, avg_tacc))\n",
        "\n",
        "results\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
